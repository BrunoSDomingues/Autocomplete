{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d456c12f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25f76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import glob, re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b4cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/home/brunosd/Documents/Insper/7o_Semestre/NLP/Autocomplete/data'\n",
    "all_files = glob.glob(path + \"/english_sentences*.csv\")\n",
    "\n",
    "# Deletes previous csv files to avoid errors in overwriting\n",
    "for file in all_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d3f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(\"data/old-newspaper.tsv\", sep=\"\\t\")\n",
    "df = ddf[ddf[\"Language\"] == \"English\"]\n",
    "df = df[[\"Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26251fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/brunosd/Documents/Insper/7o_Semestre/NLP/Autocomplete/data/english_sentences0.csv',\n",
       " '/home/brunosd/Documents/Insper/7o_Semestre/NLP/Autocomplete/data/english_sentences1.csv',\n",
       " '/home/brunosd/Documents/Insper/7o_Semestre/NLP/Autocomplete/data/english_sentences2.csv',\n",
       " '/home/brunosd/Documents/Insper/7o_Semestre/NLP/Autocomplete/data/english_sentences3.csv',\n",
       " '/home/brunosd/Documents/Insper/7o_Semestre/NLP/Autocomplete/data/english_sentences4.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/47812785/remove-empty-partitions-in-dask\n",
    "def cull_empty_partitions(df):\n",
    "    ll = list(df.map_partitions(len).compute())\n",
    "    df_delayed = df.to_delayed()\n",
    "    df_delayed_new = list()\n",
    "    pempty = None\n",
    "    for ix, n in enumerate(ll):\n",
    "        if 0 == n:\n",
    "            pempty = df.get_partition(ix)\n",
    "        else:\n",
    "            df_delayed_new.append(df_delayed[ix])\n",
    "    if pempty is not None:\n",
    "        df = dd.from_delayed(df_delayed_new, meta=pempty)\n",
    "    return df\n",
    "\n",
    "dd.to_csv(df=cull_empty_partitions(df), filename=\"data/english_sentences*.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589005d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset all files after recreating\n",
    "all_files = glob.glob(path + \"/english_sentences*.csv\")\n",
    "\n",
    "# https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "data = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c089f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to string\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0403c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing urls\n",
    "def limpa_url(texto):\n",
    "    # Regex obtida de https://www.geeksforgeeks.org/python-check-url-string/\n",
    "    pattern = r\"\"\"\n",
    "        (?i)  # Ignore case.\n",
    "        \\b  # Inicio de palavra.\n",
    "        (?:\n",
    "            https?://\n",
    "        |\n",
    "            www\n",
    "            \\d{0,3}\n",
    "            [.]\n",
    "        |\n",
    "            [a-z0-9.\\-]+\n",
    "            [.]\n",
    "            [a-z]{2,4}\n",
    "            /\n",
    "        )\n",
    "        (?:\n",
    "            [^\\s()<>]+\n",
    "        |\n",
    "            \\(\n",
    "            (?:\n",
    "                [^\\s()<>]+\n",
    "            |\n",
    "                \\(\n",
    "                [^\\s()<>]+\n",
    "                \\)\n",
    "            )*\n",
    "            \\)\n",
    "        )+\n",
    "        (?:\n",
    "            \\(\n",
    "            (?:\n",
    "                [^\\s()<>]+\n",
    "            |\n",
    "                \\(\n",
    "                [^\\s()<>]+\n",
    "                \\)\n",
    "            )*\n",
    "            \\)\n",
    "        |\n",
    "            [^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]\n",
    "        )\n",
    "    \"\"\"\n",
    "    repl = \"\"\n",
    "    matcher = re.compile(pattern, re.VERBOSE)\n",
    "    return matcher.sub(repl, texto)\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: limpa_url(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b3b898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing sentences that contain \"@, #, $, %, &, *, '\"\n",
    "data = data[~data[\"Text\"].str.contains(r\"[\\@\\#\\$\\%\\&\\*\\`]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306c0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing ? and ! for .\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.replace(\"?\", \".\").replace(\"!\", \".\"))\n",
    "\n",
    "# Removing punctuation except for apostrophes and full stops\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: re.sub(r\"[^\\w\\s\\'\\.]\", \"\", t))\n",
    "\n",
    "# Replacing all apostrophes for dashes in order to keep same word (don't, i'm)\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.replace(\"\\'\", \"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f94ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting by . and exploding to generate new rows\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.split(\".\"))\n",
    "data = data.explode(\"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab79968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing trailing and leading whitespaces\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.strip())\n",
    "\n",
    "# Removes tabs, newlines and extra whitespaces\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.replace(\"/\\s\\s+/g\", \" \"))\n",
    "\n",
    "# Filtering out empty sentences and sentences that have more than 4 words and less than 10\n",
    "data = data[(data[\"Text\"] != \"\") & (data[\"Text\"].str.split(\" \").str.len() >= 7) & (data[\"Text\"].str.split(\" \").str.len() <= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81c7a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing numbers\n",
    "data[\"hasNumbers\"] = data[\"Text\"].apply(lambda t: any(char.isdigit() for char in t))\n",
    "data = data[data[\"hasNumbers\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "753ef8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase EVERYTHING\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ef0dd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is it an issue serious enough to merit their a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will it definitely not make the situation worse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the revel casino hit the jackpot here at gover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im just up there trying to make good contact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>let your hair down it looks better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>two greek words philautos and philargyros succ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the protester seeks to curb the second phenomenon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comad with the harried office worker and the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>childwellbeing indicators show disparities fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>there are health disparities in infant mortali...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  is it an issue serious enough to merit their a...\n",
       "1    will it definitely not make the situation worse\n",
       "2  the revel casino hit the jackpot here at gover...\n",
       "3       im just up there trying to make good contact\n",
       "4                 let your hair down it looks better\n",
       "5  two greek words philautos and philargyros succ...\n",
       "6  the protester seeks to curb the second phenomenon\n",
       "7  comad with the harried office worker and the c...\n",
       "8  childwellbeing indicators show disparities fro...\n",
       "9  there are health disparities in infant mortali..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resetting index\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Keeping only text column\n",
    "data = data[[\"Text\"]]\n",
    "\n",
    "# Visualize data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc14f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data/english_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44e9bb",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f3fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/english_sentences.csv\", \"r\") as file:\n",
    "    next(file)\n",
    "    sentences = [s for s in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fe8f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "unigrams = []\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "fourgrams = []\n",
    "tokenized = []\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    for word in words:\n",
    "        if word == \".\":\n",
    "            words.remove(word)\n",
    "        else:\n",
    "            unigrams.append(word)\n",
    "        \n",
    "        tokenized.append(words)\n",
    "        \n",
    "        bigrams.extend(list(ngrams(words, 2)))\n",
    "        trigrams.extend(list(ngrams(words, 3)))\n",
    "        fourgrams.extend(list(ngrams(words, 4)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "690d00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(ngram: list):\n",
    "    new_ngram = []\n",
    "    for sequence in ngram:\n",
    "        count = 0\n",
    "        for word in sequence:\n",
    "            count = count or 0 if word in stop_words else count or 1\n",
    "        if count == 1:\n",
    "            new_ngram.append(sequence)\n",
    "            \n",
    "    return new_ngram\n",
    "\n",
    "bigrams = remove_stopwords(bigrams)\n",
    "trigrams = remove_stopwords(trigrams)\n",
    "fourgrams = remove_stopwords(fourgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc9122",
   "metadata": {},
   "source": [
    "## Probabilidades e Smoothing\n",
    "\n",
    "Probabilidade de uma palavra no texto:\n",
    "\n",
    "$\\large{P(palavra) = \\frac{n_{ocorrencias}}{N_{palavras}}}$\n",
    "\n",
    "Se a palavra não existe, precisamos adicionar artificialmente para não gerar probabilidade zero. Portanto, adiciona-se 1 (Laplace Smoothing) a todos os membros do vocabulário. Como adiciona-se 1, apenas precisamos contar quantas palavras únicas temos no dicionário para considerar os 1 somados.\n",
    "\n",
    "$\\large{P(palavra) = \\frac{n_{ocorrencias} + 1}{N_{palavras} + N_{unicas}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddbf8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "n1, n2, n3, n4 = Counter(unigrams), Counter(bigrams), Counter(trigrams), Counter(fourgrams)\n",
    "s1, s2, s3, s4 = set(unigrams), set(bigrams), set(trigrams), set(fourgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05e95014",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = [[n, (n1[n] + 1)/(len(n1) + len(s1))] for n in s1]\n",
    "p1 = sorted(p1, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4447babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = [[n, (n2[n] + 1)/(len(n2) + len(s2))] for n in s2]\n",
    "p2 = sorted(p2, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1872df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = [[n, (n3[n] + 1)/(len(n3) + len(s3))] for n in s3]\n",
    "p3 = sorted(p3, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec0c5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = [[n, (n4[n] + 1)/(len(n4) + len(s4))] for n in s4]\n",
    "p4 = sorted(p4, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef816a9a",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed6557ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not form a n-gram of size 3 with given input.\n"
     ]
    }
   ],
   "source": [
    "example = \"buy\"\n",
    "\n",
    "def return_prediction(inp, n_words, model=2):\n",
    "    # Prediction list\n",
    "    pred = []\n",
    "    \n",
    "    # Tokenize our input\n",
    "    inp = word_tokenize(inp)\n",
    "    \n",
    "    # Probability model\n",
    "    p = None\n",
    "    \n",
    "    # Ngram\n",
    "    ngram = None\n",
    "    \n",
    "    if model == 2:\n",
    "        p = p2\n",
    "        \n",
    "    elif model == 3:\n",
    "        p = p3\n",
    "                \n",
    "    elif model == 4:\n",
    "        p = p4\n",
    "    \n",
    "    if len(inp) < model - 1:\n",
    "        return f\"Could not form a n-gram of size {model} with given input.\"\n",
    "    \n",
    "    # Select n - 1 last words per ngram type\n",
    "    ngram = list(ngrams(inp, model - 1))[-1] \n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for wp in p:\n",
    "        # Cut off last word of ngram prob and see if matches with the last word of the ngram generated\n",
    "        if wp[0][:-1] == ngram:\n",
    "            count += 1\n",
    "            pred.append(wp[0][-1].replace(\"-\", \"'\"))\n",
    "            \n",
    "            if count == n_words:\n",
    "                break\n",
    "                \n",
    "    if count < 5:\n",
    "        pred += [\"N/A\"] * (n_words - count)\n",
    "        \n",
    "    return pred\n",
    "\n",
    "pred = return_prediction(example, n_words=5, model=3)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
