{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6510c4",
   "metadata": {},
   "source": [
    "# Autosuggest\n",
    "\n",
    "- Proposta similar a da search engine do Google (dado palavras em um input, qual a próxima palavra que mais se encaixa?)\n",
    "- Uso de dataset de jornais\n",
    "- Focado para língua inglesa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456c12f",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "- Uso do Dask para ler os dados iniciais\n",
    "- Uso do Pandas para o dataframe da língua inglesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25f76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import glob, re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b4cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path dos dados\n",
    "path = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "# Path dos arquivos pré-existentes\n",
    "all_files = glob.glob(path + \"/english_sentences*.csv\")\n",
    "\n",
    "# Deletando arquivos csv pré-existentes\n",
    "for file in all_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d3f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo os dados do arquivo tsv\n",
    "ddf = dd.read_csv(\"data/old-newspaper.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Selecionando somente em inglês e somente a coluna Text, pois os outros dados não serão utilizados\n",
    "df = ddf[ddf[\"Language\"] == \"English\"]\n",
    "df = df[[\"Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cab25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpeza de particoes vazias\n",
    "# https://stackoverflow.com/questions/47812785/remove-empty-partitions-in-dask\n",
    "def cull_empty_partitions(df):\n",
    "    ll = list(df.map_partitions(len).compute())\n",
    "    df_delayed = df.to_delayed()\n",
    "    df_delayed_new = list()\n",
    "    pempty = None\n",
    "    \n",
    "    for ix, n in enumerate(ll):\n",
    "        if 0 == n:\n",
    "            pempty = df.get_partition(ix)\n",
    "            \n",
    "        else:\n",
    "            df_delayed_new.append(df_delayed[ix])\n",
    "            \n",
    "    if pempty is not None:\n",
    "        df = dd.from_delayed(df_delayed_new, meta=pempty)\n",
    "        \n",
    "    return df\n",
    "\n",
    "df = cull_empty_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c8f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo em um dataframe do pandas novamente\n",
    "data = df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db940c3f",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- É necessário fazer a limpeza dos dados.\n",
    "- Há caracteres diversos, possíveis URLs, entre outros que devem ser removidos da frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881e393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo URLs\n",
    "def limpa_url(texto):\n",
    "    # Regex obtida de https://www.geeksforgeeks.org/python-check-url-string/\n",
    "    pattern = r\"\"\"\n",
    "        (?i)  # Ignore case.\n",
    "        \\b  # Inicio de palavra.\n",
    "        (?:\n",
    "            https?://\n",
    "        |\n",
    "            www\n",
    "            \\d{0,3}\n",
    "            [.]\n",
    "        |\n",
    "            [a-z0-9.\\-]+\n",
    "            [.]\n",
    "            [a-z]{2,4}\n",
    "            /\n",
    "        )\n",
    "        (?:\n",
    "            [^\\s()<>]+\n",
    "        |\n",
    "            \\(\n",
    "            (?:\n",
    "                [^\\s()<>]+\n",
    "            |\n",
    "                \\(\n",
    "                [^\\s()<>]+\n",
    "                \\)\n",
    "            )*\n",
    "            \\)\n",
    "        )+\n",
    "        (?:\n",
    "            \\(\n",
    "            (?:\n",
    "                [^\\s()<>]+\n",
    "            |\n",
    "                \\(\n",
    "                [^\\s()<>]+\n",
    "                \\)\n",
    "            )*\n",
    "            \\)\n",
    "        |\n",
    "            [^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]\n",
    "        )\n",
    "    \"\"\"\n",
    "    repl = \"\"\n",
    "    matcher = re.compile(pattern, re.VERBOSE)\n",
    "        \n",
    "    return matcher.sub(repl, texto)\n",
    "\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: limpa_url(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3b898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo caracteres especiais \"@, #, $, %, &, *, `\"\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: re.sub(r\"[\\@\\#\\$\\%\\&\\*\\`]\", \"\", t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "306c0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo replace de ? e ! por .\n",
    "# Esses caracteres indicam o fim de uma frase, mudando apenas sua entonação.\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.replace(\"?\", \".\").replace(\"!\", \".\"))\n",
    "\n",
    "# Removendo pontuações diversas exceto apostrofes e pontos finais\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: re.sub(r\"[^\\w\\s\\'\\.]\", \"\", t))\n",
    "\n",
    "# Replace de apostrofes por traços para conservar palavras como don't e i'm na hora de tokenizar\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.replace(\"\\'\", \"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f94ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo o split das frases por .\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.split(\".\"))\n",
    "\n",
    "# Comando explode para separar as listas em novas linhas do dataframe\n",
    "data = data.explode(\"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab79968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo espaços em branco no início e no fim da frase\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.strip())\n",
    "\n",
    "# Removendo tabs, newlines e espaços em branco em excesso\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.replace(\"/\\s\\s+/g\", \" \"))\n",
    "\n",
    "# Removendo frases que contém números\n",
    "data[\"hasNumbers\"] = data[\"Text\"].apply(lambda t: any(char.isdigit() for char in t))\n",
    "data = data[data[\"hasNumbers\"] == False]\n",
    "\n",
    "# Removendo frases vazias e mantendo frases que possuem entre 7 e 12 palavras\n",
    "data = data[(data[\"Text\"] != \"\") & (data[\"Text\"].str.split(\" \").str.len() >= 7) & (data[\"Text\"].str.split(\" \").str.len() <= 12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "753ef8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "data[\"Text\"] = data[\"Text\"].apply(lambda t: t.lower())\n",
    "\n",
    "# Reset index\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Transforming to list\n",
    "sentences = data[\"Text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44e9bb",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "- após processar os dados, utilizamos o NLTK para:\n",
    "    - Tokenizacao\n",
    "    - Uso de stopwords\n",
    "    - Criacao de ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5b0ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe8f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dos ngramas\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "fourgrams = []\n",
    "\n",
    "# Stopwords da língua inglesa\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Iterando sobre cada frase\n",
    "for sentence in sentences:\n",
    "    # Tokenizando as frases\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Extendendo a lista dos ngramas com as palavras\n",
    "    bigrams.extend(list(ngrams(words, 2)))\n",
    "    trigrams.extend(list(ngrams(words, 3)))\n",
    "    fourgrams.extend(list(ngrams(words, 4)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "690d00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ngramas que são compostos exclusivamente por stopwords\n",
    "def remove_stopwords(ngram: list):\n",
    "    new_ngram = []\n",
    "    for sequence in ngram:\n",
    "        count = 0\n",
    "        for word in sequence:\n",
    "            count = count or 0 if word in stop_words else count or 1\n",
    "        if count == 1:\n",
    "            new_ngram.append(sequence)\n",
    "            \n",
    "    return new_ngram\n",
    "\n",
    "bigrams = remove_stopwords(bigrams)\n",
    "trigrams = remove_stopwords(trigrams)\n",
    "fourgrams = remove_stopwords(fourgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc9122",
   "metadata": {},
   "source": [
    "## Probabilidades e Smoothing\n",
    "\n",
    "Probabilidade de uma palavra no texto:\n",
    "\n",
    "$\\large{P(palavra) = \\frac{n_{ocorrencias}}{N_{palavras}}}$\n",
    "\n",
    "Se a palavra não existe, precisamos adicionar artificialmente para não gerar probabilidade zero. Portanto, adiciona-se 1 (Laplace Smoothing) a todos os membros do vocabulário. Como adiciona-se 1, apenas precisamos contar quantas palavras únicas temos no dicionário para considerar os 1 somados.\n",
    "\n",
    "$\\large{P(palavra) = \\frac{n_{ocorrencias} + 1}{N_{palavras} + N_{unicas}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddbf8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "n2, n3, n4 = Counter(bigrams), Counter(trigrams), Counter(fourgrams)\n",
    "s2, s3, s4 = set(bigrams), set(trigrams), set(fourgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4447babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando a fórmula de probabilidades\n",
    "p2 = [(n, (n2[n] + 1)/(len(n2) + len(s2))) for n in s2]\n",
    "p3 = [(n, (n3[n] + 1)/(len(n3) + len(s3))) for n in s3]\n",
    "p4 = [(n, (n4[n] + 1)/(len(n4) + len(s4))) for n in s4]\n",
    "\n",
    "# Ordena-se as listas de acordo com a probabilidade de cada N-grama em ordem descrescente\n",
    "p2 = sorted(p2, key=lambda w: w[1], reverse=True)\n",
    "p3 = sorted(p3, key=lambda w: w[1], reverse=True)\n",
    "p4 = sorted(p4, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef816a9a",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fef5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(inp, n, model=2):\n",
    "    # Lista de possiveis frases\n",
    "    pred = []\n",
    "    \n",
    "    # Tokenize\n",
    "    inp = word_tokenize(inp)\n",
    "    \n",
    "    # Modelo escolhido\n",
    "    p = None\n",
    "    \n",
    "    # Ngram\n",
    "    ngram = None\n",
    "    \n",
    "    if model == 2:\n",
    "        p = p2\n",
    "        \n",
    "    elif model == 3:\n",
    "        p = p3\n",
    "                \n",
    "    elif model == 4:\n",
    "        p = p4\n",
    "    \n",
    "    # Se o tamanho do input for menor que o modelo escolhido, não é possível formar um ngrama.\n",
    "    # Logo, devemos retornar um erro.\n",
    "    if len(inp) < model - 1:\n",
    "        return f\"Could not form a n-gram of size {model} with given input.\"\n",
    "\n",
    "    # Forma-se os ngramas de tamanho model - 1, e pega-se o último\n",
    "    ngram = list(ngrams(inp, model - 1))[-1]\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for wp in p:\n",
    "        # Pega-se os ngramas do modelo escolhido, remove-se a probabilidade\n",
    "        # Corta-se a última palavra para tentar descobrir um ngrama que dá match com o nosso ngrama\n",
    "        if wp[0][:-1] == ngram:\n",
    "            count += 1\n",
    "            # Adiciona-se a última palavra, pois houve um match\n",
    "            \n",
    "            # O replace serve para caso encontre o -, pois ele foi inserido artificialmente\n",
    "            # para garantir os casos com apostrofe\n",
    "            pred.append(wp[0][-1].replace(\"-\", \"'\"))\n",
    "            \n",
    "            # n é o número de sugestões\n",
    "            if count == n:\n",
    "                break\n",
    "                \n",
    "    # Se não achou, preenche com N/A\n",
    "    if count < n:\n",
    "        pred += [\"N/A\"] * (n - count)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6068a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i want to be\n",
      "i want to see\n",
      "i want to do\n",
      "i want to go\n",
      "i want to make\n"
     ]
    }
   ],
   "source": [
    "example = \"i want to\"\n",
    "pred = return_prediction(example, n=5, model=3)\n",
    "\n",
    "for word in pred:\n",
    "    print(example + \" \" + word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed6557ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let me see your N/A\n",
      "let me see your N/A\n",
      "let me see your N/A\n",
      "let me see your N/A\n",
      "let me see your N/A\n"
     ]
    }
   ],
   "source": [
    "example = \"let me see your\"\n",
    "pred = return_prediction(example, n=5, model=4)\n",
    "\n",
    "for word in pred:\n",
    "    print(example + \" \" + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b47b5",
   "metadata": {},
   "source": [
    "## Futuras iterações\n",
    "\n",
    "- Trocar para a língua portuguesa\n",
    "    - Novo preprocessing\n",
    "    - Novas stopwords\n",
    "\n",
    "\n",
    "- Input em tempo real\n",
    "\n",
    "\n",
    "- Modelo com mais N-gramas\n",
    "    - Retrabalhar o dataset para não gerar muitos cortes\n",
    "\n",
    "\n",
    "- Frases não são analisadas como um todo, apenas o último n-grama e gera o match\n",
    "\n",
    "\n",
    "- Faltaria retroalimentação (muitos casos N/A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
